---
title: "Literature Review: Selection, QTL Mapping and Statistical Models"
author: "Shiyan Miao|u8027892"
date: "2025-09-08"
format:
  pdf:
    number-sections: true 
    include-in-header: 
      text: |
        \usepackage{setspace}
        \doublespacing
        \usepackage[left,pagewise]{lineno}
        \linenumbers
        \usepackage[margin=1in]{geometry}
bibliography: reference.bib
csl: chicago-author-date.csl
toc: true
---

# Introduction

Selective breeding has long been the cornerstone of agricultural improvement, enabling the development of crop varieties and livestock breeds with enhanced productivity, resilience, and quality traits to meet the growing global demand for food [@falconer1996; @godfray2010]. However, traditional approaches such as phenotypic selection, progeny testing, and backcrossing are inherently slow and resource-intensive. They rely heavily on the expression of observable traits, which are often influenced by environmental variation, and thus provide only an indirect and delayed measure of the underlying genetic potential [@collard2005]. 

Marker-assisted selection (MAS) emerged as a transformative strategy to accelerate the breeding process by integrating molecular markers into selection decisions. Because DNA markers are not affected by environmental factors and can be assayed at early developmental stages, MAS allows breeders to identify and retain individuals carrying favorable alleles long before the phenotype is expressed [@collard2005; @hasan2021]. Importantly, the success of MAS depends on the prior identification of molecular markers that are tightly linked to quantitative trait loci (QTLs), the genomic regions underlying phenotypic variation in complex traits [@mackay2009]. Establishing these marker–trait associations is therefore a critical step in harnessing MAS for crop and livestock improvement.

## Selection in Biology

Selection in genetics refers to the process by which individuals with desirable traits are chosen as parents to produce the next generation, thereby ensuring the inheritance of favourable alleles. Falconer and Mackay [@falconer1996, p. 188] formally defined selection as the differential reproduction of genotypes, either due to environmental pressures (natural selection) or through breeder-imposed choices (artificial selection). Natural selection reflects the influence of the environment on survival and reproductive success, whereas artificial selection relies on human intervention to propagate traits of agronomic or economic importance.

Three major traditional approaches have historically been employed in plant and animal breeding:

-   **Phenotypic selection**: This approach selects individuals based on observable characteristics or performance. While straightforward, its accuracy is reduced by the confounding influence of environmental factors, which obscure genetic differences. Collard et al. [@collard2005, p. 170] emphasise that phenotypic selection is limited in efficiency, particularly for traits with low heritability, late expression, or strong environmental dependence.

-   **Progeny testing**: In cases where traits cannot be assessed in the parent generation, selection is based on the measured performance of offspring. Falconer and Mackay [@foster2006, p. 13] note that progeny testing provides more reliable genetic evaluation but is resource-intensive, time-consuming, and often financially prohibitive.

-   **Backcross breeding**: This method involves introgressing a favourable allele from a donor into an elite genetic background through repeated backcrossing. Wu, Ma, and Casella [@wu2007, p.4] explain that while effective, backcrossing requires six to eight generations and is hampered by linkage drag, whereby undesirable alleles linked to the favourable gene are also inherited.

## Marker-Assisted Selection (MAS)

Marker-assisted selection (MAS) represents a methodological advance that addresses the inefficiencies of conventional methods. MAS uses DNA markers tightly linked to genes of interest as proxies for phenotypic traits, allowing for selection at the seedling stage and independent of environmental variation. Collard et al. [@collard2005, pp. 184–187] highlight MAS as a more precise, reliable, and cost-effective strategy, especially for traits with low heritability or late expression. More recently, Hasan et al. [@hasan2021, pp. 9-10] stress that MAS enables the pyramiding of multiple beneficial alleles, offering significant potential for crop improvement.

## Quantitative Trait Loci (QTL)

**Definition of QTL** Quantitative trait loci (QTL) are defined as genomic regions containing one or more genes that contribute to variation in quantitative traits. Mackay, Stone, and Ayroles [@mackay2009, p. 565] describe a QTL as a segment of DNA statistically associated with phenotypic variation through its linkage with polymorphic markers. Similarly, Collard et al. [@collard2005, pp. 169-170] emphasise that QTLs represent chromosomal regions rather than single genes, and are identified via their effects on polygenic, complex traits such as yield or disease resistance.

**Methods for Finding QTL** The identification of QTL relies on the statistical association between marker genotypes and phenotypic traits. Classical approaches include linkage mapping, which traces the co-segregation of markers and traits in experimental populations, and association mapping, which utilises historical recombination events in natural populations [@collard2005, p. 170]. The landmark interval mapping method developed by Lander and Botstein [@lander1989,pp. 188-189] introduced likelihood-based estimation of QTL positions between flanking markers, a framework that has since been expanded into mixture model [@wu2007, pp. 203–204].

**QTL Data Structure** In practice, QTL analyses are based on a structured dataset comprising marker genotypes and quantitative phenotypes. Genotypes are typically coded numerically (e.g., 0 = homozygous for one allele, 1 = homozygous for the alternative allele), allowing direct statistical modelling of their association with phenotypic values [@mackay2009, p. 565]. Because of financial and technological constraints, whole-genome sequencing is not always feasible, and QTL are often inferred from linkage disequilibrium between genotyped markers and unobserved causal loci [@collard2005, pp. 171–172].

**Linkage and Recombination**  The principle of linkage is central to QTL mapping. Two loci located on the same chromosome are said to be linked when they are inherited together more frequently than expected under independent assortment. Recombination frequency \( r \) quantifies the likelihood of crossover events between loci: \( r = 0 \) indicates complete linkage (no recombination), while \( r = 0.5 \) corresponds to independent segregation [@falconer1996, p. 60]. In practice, tightly linked markers serve as proxies for nearby QTL, as recombination events between them are rare, thereby allowing geneticists to map phenotypic variation to specific chromosomal regions [@mackay2009, p. 566].

# Method

The detection of quantitative trait loci (QTL) fundamentally relies on the development and application of appropriate statistical models. These models provide a framework to relate marker genotypes to phenotypic variation and to identify genomic regions that contribute to quantitative traits. We begin by examining the statistical foundations of QTL detection, focusing first on the single-marker model. In the context of a backcross population, this model allows us to evaluate the recombination frequency between a marker and a putative QTL, to express the relationship as a univariate regression, and to extend inference through mixture distribution approaches that accommodate uncertainty in underlying genotypes. 

Building on this, we then turn to the multiple marker model, which generalises single-locus regression to incorporate multiple loci simultaneously. This formulation enhances power and reduces spurious associations but also introduces challenges related to multicollinearity and high-dimensionality. To address these issues, three principal strategies of model selection are considered. The first is **stepwise selection**, which employs criteria such as the AIC and BIC to iteratively identify an optimal subset of markers. The second is **Bayesian shrinkage**, in which hierarchical priors induce marker-specific shrinkage, enabling adaptive sparsity and improved estimation. The third is **penalised selection**, which encompasses ridge regression, the least absolute shrinkage and selection operator (LASSO), and the elastic net. These approaches introduce regularisation penalties that mitigate overfitting, stabilise estimation in the presence of linkage disequilibrium, and achieve a balance between variable selection and grouping effects. 

Together, these models and strategies form the methodological basis for contemporary QTL mapping and marker-assisted selection.


## Statistical Models for Detecting QTL

Statistical models are central to quantitative trait locus (QTL) analysis, providing the formal link between observed phenotypic variation and underlying genetic architecture. A QTL is defined as a chromosomal region containing one or more loci that contribute to variation in a quantitative trait [@mackay2009]. Because QTLs cannot be observed directly, their presence and effects must be inferred statistically by evaluating associations between genetic markers and phenotypic outcomes in a mapping population. Statistical models provide the framework for this inference by decomposing the phenotype into systematic components attributable to genetic and environmental factors and by quantifying the extent to which marker genotypes predict trait values [@falconer1996].

At their core, these models specify the relationship between a phenotypic response variable and explanatory variables that represent marker genotypes. 
$$
y_i = x_i + z_i + x_i \times z_i + \epsilon_i,
$$

where $y_i$ is the phenotype of the $i$-th individual, $x_i$ the genetic effect, $z_i$ the environmental effect, and $\epsilon_i$ the residual error. For the purposes of this study, the model is simplified to focus exclusively on the genetic contribution to phenotypic variation:

$$
y_i = x_i + \epsilon_i.
$$
[@falconer1996, pp. 111–112].

The underlying assumption of this simplification is that environmental effects and their interactions with genotype are negligible. Accordingly, the analysis considers only the phenotypic variance attributable to genetic effects, with the residual term capturing unexplained variation.


## Single Marker Model

### Backcross population

Backcross (BC) populations are generated by crossing an F₁ hybrid back to one of its parental lines. In the illustrated scheme, an individual homozygous for allele A (blue, with a star marker) is crossed with an individual homozygous for allele C (orange, with a star marker). The F₁ generation inherits one allele from each parent and is then crossed again with the recurrent parent carrying allele C.

![](Figure%201.png){#fig-1 fig-cap="Figure 1. Backcross scheme showing recombination frequency" fig-align="center" width=70%}


As shown in the figure, recombination between the marker locus and the putative QTL occurs with probability $c$, the recombination fraction. The resulting BC₁ progeny segregate into four possible genotypes:

With probability $\tfrac{1}{2}(1-c)$, the offspring inherit the non-recombinant haplotype associated with genotypic value $d$.

With probability $\tfrac{1}{2}(1-c)$, they inherit the alternate non-recombinant haplotype with genotypic value $a$.

With probability $\tfrac{1}{2}c$, recombination produces a haplotype with genotypic value $a$.

With probability $\tfrac{1}{2}c$, recombination produces a haplotype with genotypic value $d$.

Thus, the figure illustrates how the distribution of progeny genotypes in a backcross depends on recombination between the marker and QTL. When $c=0$, the marker and QTL are perfectly linked, and segregation follows the expected 1:1 ratio of parental haplotypes. When $c=0.5$, the marker and QTL assort independently, and no linkage information can be extracted.

In statistical terms, the backcross design is particularly powerful because it simplifies the genetic background: only two marker classes are present (AC vs CC). This makes it possible to test the association between marker genotype and phenotype using a univariate regression or a t-test, providing a straightforward framework for detecting linkage between markers and QTL.

### Recombination Frequency

From the difference formula:

$$
\Delta = (d-a)(1-2c)
$$

1. If \(d-a = 0\): no genetic effect.  
2. If \(1-2c = 0 \Rightarrow c = 0.5\): the marker and QTL are unlinked [@yang2019].

### Univariate regression

For an F2 population, the expected mean of genotype **AC** is:

$$
\mu_{AC} = \tfrac{1}{2}(1-c)\,d + \tfrac{1}{2}c\,a
$$

and the expected mean of genotype **CC** is:

$$
\mu_{CC} = \tfrac{1}{2}(1-c)\,a + \tfrac{1}{2}c\,d
$$

Thus, the difference is:

$$
\Delta = \mu_{AC} - \mu_{CC} = (d-a)(1-2c)
$$
[@yang2019].

In the context of a backcross (BC) population, where each locus segregates into only two possible genotypes (AC and CC), the single-marker regression model can be specified as:

$$
y_i = \mu + \beta G_i + \epsilon_i,
$$

where  

- $y_i$ is the observed phenotype of the $i$-th individual,  
- $G_i$ is the coded marker genotype, taking values $0$ for AC and $1$ for CC,  
- $\mu$ is the overall population mean,  
- $\beta$ represents the effect of the marker, defined as the mean difference between the two genotypic classes,  
- explicitly, $\beta = \mu_{AC} - \mu_{CC} = (d-a)(1-2c)$, with $a$ denoting the additive effect, $d$ the dominance effect, and $c$ the recombination fraction between the marker and the QTL,  
- $\epsilon_i \sim \mathcal{N}(0,\sigma^2)$ is the residual error term.  

This formulation treats the single marker as a predictor variable in a simple linear regression model, analogous to the role of multiple markers in a multiple regression framework. The regression coefficient $\beta$ captures the expected phenotypic shift associated with the marker genotype, conditional on recombination with the underlying QTL.


This allows testing for marker–QTL association using a t-test [@foster2006, p. 15].

### Mixture Distribution

In a BC1 population:

- If marker genotype is AC:

$$
P(Qq \mid AC) = 1-c, \qquad P(qq \mid AC) = c
$$

- If marker genotype is CC:

$$
P(Qq \mid CC) = c, \qquad P(qq \mid CC) = 1-c
$$
[@foster2006, p. 15].

The phenotype density conditional on marker genotype is:

$$
f(z \mid AC) = (1-c)\,\phi\!\left(z;\mu_{Qq},\sigma^2\right) + c\,\phi\!\left(z;\mu_{qq},\sigma^2\right)
$$

$$
f(z \mid CC) = c\,\phi\!\left(z;\mu_{Qq},\sigma^2\right) + (1-c)\,\phi\!\left(z;\mu_{qq},\sigma^2\right)
$$

where the normal density is

$$
\varphi(z \mid \mu,\sigma^2) \;=\; \frac{1}{\sqrt{2\pi\sigma^2}}\,
\exp\!\left(-\frac{(z-\mu)^2}{2\sigma^2}\right).
$$

The overall phenotypic distribution is therefore a two-component normal mixture [@wu2007, pp. 204–210; @foster2006, p. 15].

The symbol $z$ denotes the observed value of the variable, which in QTL analysis can be understood as an individual’s measured phenotype (e.g., plant height or yield). The symbol $\mu$ represents the mean of the distribution, corresponding to the expected phenotype or genotypic value for a given marker genotype. The symbol $\sigma^2$ indicates the variance of the distribution, capturing the spread of observations around the mean; in QTL models it reflects residual variation not explained by genotype, such as environmental influences or measurement error.


## Multiple Marker Model

In contrast to single-marker analysis, which tests one locus at a time, the multiple marker model simultaneously incorporates information from several markers across the genome into a linear regression framework. This approach improves statistical power by accounting for background loci and reduces spurious associations that may arise when only one marker is analysed in isolation.

Formally, the model can be expressed as:

$$
y_i = \mu + \sum_{j=1}^{m} \beta_j G_{ij} + \epsilon_i,
$$

where  

- $y_i$ is the phenotypic value of the $i$-th individual,  
- $G_{ij}$ denotes the coded genotype of the $j$-th marker for the $i$-th individual (for example, 0/1 coding for backcross populations; 0/1/2 coding for F$_2$ populations),  
- $\beta_j$ represents the partial regression coefficient, capturing the effect of marker $j$ conditional on the presence of other markers in the model,  
- $\mu$ is the overall mean, and  
- $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$ denotes the residual error term, assumed to follow an independent and identically distributed normal distribution.  

Classical approaches in multiple marker model include **stepwise regression** and model selection based on information criteria such as the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). More recently, **penalised regression methods** have become widely adopted. Ridge regression applies an $\ell_2$ penalty to shrink coefficients and stabilize estimates in the presence of multicollinearity [@ogutu2012], while the least absolute shrinkage and selection operator (LASSO) employs an $\ell_1$ penalty that simultaneously performs variable selection and regularization [@li2012]. The elastic net combines both penalties, offering a compromise between selection and stability, which is particularly advantageous in high-dimensional settings where markers are highly correlated [@zou2005; @brault2021].


we will introduce each of these methods in details for the rest of the section. 

### Stepwise Selection

A classical approach to variable selection is stepwise regression, in which markers are iteratively added or removed from the model according to predefined criteria. In practice, the AIC and BIC are widely used to select the model that minimises information loss:

$$
\text{AIC} = -2 \log L + 2k, \qquad 
\text{BIC} = -2 \log L + k \log n,
$$

where $L$ is the likelihood of the model, $k$ the number of parameters, and $n$ the sample size.  

Stepwise procedures are computationally straightforward and interpretable. However, they are known to be unstable in high-dimensional genomic contexts and fail to account for model uncertainty, often leading to overconfident inference [@foster2006].

### Bayesian Shrinkage

A more flexible alternative is Bayesian shrinkage, in which regression coefficients are assigned hierarchical priors that adaptively control the degree of shrinkage for each marker. For marker effect $\beta_j$, a zero-mean normal prior is assumed:

$$
\beta_j \sim \mathcal{N}\!\left(0, \frac{\sigma^2}{\lambda_j}\right),
$$

where $\lambda_j$ is a marker-specific precision parameter. To complete the hierarchy, a Gamma prior is placed on $\lambda_j$:

$$
\lambda_j \sim \text{Gamma}(a,b).
$$

This normal–gamma prior structure implies that larger values of $\lambda_j$ lead to stronger shrinkage, pulling $\beta_j$ closer to zero, whereas smaller values allow larger deviations. Consequently, Bayesian shrinkage achieves **adaptive sparsity**: most noise markers are heavily shrunk, while a small number of true signals are retained. This feature makes the approach particularly attractive for genome-wide QTL mapping [@xu2003].

### Penalised Regression: Ridge, LASSO, and Elastic Net

Penalised regression provides a frequentist counterpart to Bayesian shrinkage by incorporating regularisation terms into the optimisation problem. The general form is:

$$
\widehat{\boldsymbol{\beta}}
=
\arg\min_{\boldsymbol{\beta}}
\left\{
\frac{1}{2n}\bigl\|\mathbf{y}-\mu\mathbf{1}-\mathbf{X}\boldsymbol{\beta}\bigr\|_2^2
+ \lambda_1 \|\boldsymbol{\beta}\|_1
+ \lambda_2 \|\boldsymbol{\beta}\|_2^2
\right\}.
$$
In penalised regression frameworks for QTL mapping, two hyperparameters, $\lambda_1$ and $\lambda_2$, govern the strength of regularisation. Specifically, $\lambda_1$ controls the $\ell_1$ penalty that induces sparsity by shrinking some coefficients exactly to zero, while $\lambda_2$ controls the $\ell_2$ penalty that shrinks coefficients toward zero without eliminating them. 

- **Ridge regression** ($\lambda_2>0, \lambda_1=0$): shrinks all coefficients toward zero but not exactly to zero. It stabilises estimates under strong multicollinearity, especially when markers are in linkage disequilibrium (LD). However, ridge does not perform variable selection.  

- **LASSO** ($\lambda_1>0, \lambda_2=0$): uses $\ell_1$ shrinkage, which forces some coefficients exactly to zero, thereby achieving variable selection. Its limitation lies in instability when markers are highly correlated; it tends to arbitrarily select one marker and discard the rest.  

- **Elastic net** ($\lambda_1>0, \lambda_2>0$): combines $\ell_1$ and $\ell_2$ penalties. The $\ell_2$ component induces a **grouping effect**, whereby correlated markers are either included or excluded together, while the $\ell_1$ component ensures sparsity by eliminating irrelevant variables. Elastic net is particularly suitable for genomic data where markers cluster in LD blocks [@wu2007].

In summary, stepwise selection, Bayesian shrinkage, and penalised regression represent complementary strategies for addressing the limitations of multiple marker models. Stepwise methods are simple but unstable; Bayesian shrinkage provides adaptive, marker-specific shrinkage; and penalised regression approaches offer computationally efficient solutions for high-dimensional problems, with the elastic net providing the most balanced performance in the presence of strong marker correlations.

# Discussion and Summary

This review has outlined the development of breeding approaches from traditional phenotype-based selection to marker-assisted selection (MAS) and, more recently, statistical models for QTL detection. While these advances have accelerated breeding efficiency, important challenges remain. First, QTL often map to broad genomic regions rather than specific genes, which restricts biological interpretation. Second, the statistical power to detect loci is reduced when traits are of low heritability or when marker density is insufficient to capture genetic variation. Third, QTL studies are almost always performed with finite sample sizes, and small experimental populations in particular limit both discovery and reproducibility. Although high-density genotyping and sequencing costs can still present barriers, these technological constraints are increasingly secondary compared to the statistical limitations.

Looking forward, future research should focus on advancing statistical methodology. One promising direction is the extension of multiple marker models to incorporate mixture distributions, which would allow more flexible modelling of heterogeneous genetic effects across loci. Another priority is to establish guidelines for minimum effective sample sizes that ensure robust QTL detection under different genetic architectures. Together, these efforts could significantly improve the resolution and reliability of QTL mapping, ultimately strengthening the role of statistical methods in modern breeding programmes.


## Single marker model vs. multiple marker model

Single marker 

In summary, the multiple marker model represents a natural extension of single marker regression, providing a more powerful and robust framework for QTL mapping. By explicitly modelling the effects of multiple loci, it allows for a more accurate dissection of the genetic architecture of complex traits. Nonetheless, the practical implementation of this model requires careful attention to issues of collinearity, dimensionality, and model selection, motivating the use of penalised regression approaches that are now standard in modern quantitative genetics and genomic selection research.

Although the multiple marker model offers conceptual and practical advantages, it also introduces new statistical challenges. One important issue is **multicollinearity** among markers, which arises because markers in close physical proximity are often in linkage disequilibrium. This correlation can inflate the variances of the estimated regression coefficients, thereby reducing interpretability and statistical reliability. Furthermore, in modern genomic studies the number of markers ($p$) frequently exceeds the number of individuals ($n$), rendering ordinary least squares estimation unstable or infeasible. Aside from multicollinearity, overfitting, and instability in high-dimensional settings where the number of markers greatly exceeds the number of samples are also potneial issue for multiple marker model [@foster2006; @wu2007].. 

## Model choice

When only a few QTL with moderate to large effects are expected, particularly in a backcross design requiring rapid screening, the single-marker model provides an efficient and appropriate approach. By contrast, when the genetic architecture involves many loci with small to medium effects, a multiple marker model is more suitable, as it enables the estimation of conditional effects while accounting for the influence of other loci. To further enhance the performance of multiple marker models, penalised regression strategies can be incorporated. In particular, the elastic net offers a balanced framework that combines the sparsity of $\ell_1$ regularisation with the grouping effect of $\ell_2$ penalties. This dual property allows for more stable selection of correlated markers while simultaneously mitigating overfitting through shrinkage.

## Future work

Looking forward, future research should focus on advancing statistical methodology. One promising direction is the extension of multiple marker models to incorporate mixture distributions, which would allow more flexible modelling of heterogeneous genetic effects across loci. Another priority is to establish guidelines for minimum effective sample sizes that ensure robust QTL detection under different genetic architectures. Together, these efforts could significantly improve the resolution and reliability of QTL mapping, ultimately strengthening the role of statistical methods in modern breeding programmes.

# Reference